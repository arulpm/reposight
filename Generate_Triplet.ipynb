{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPvVVLd0rsz1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install openai==1.55.3 httpx==0.27.2 datasets --force-reinstall --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3NQkS3xJKc-",
        "outputId": "42caf2d3-00ae-4709-c3de-6a6ac16a16aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m301.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, sentence-transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.3\n",
            "    Uninstalling transformers-4.46.3:\n",
            "      Successfully uninstalled transformers-4.46.3\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 3.2.1\n",
            "    Uninstalling sentence-transformers-3.2.1:\n",
            "      Successfully uninstalled sentence-transformers-3.2.1\n",
            "Successfully installed sentence-transformers-3.3.1 tokenizers-0.21.0 transformers-4.47.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers sentence-transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGFt8P8irvs9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5axLg9n1z0Cq",
        "outputId": "3e53d586-5bb0-434e-a0fa-bcda8b823a0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0gxpX46G3xK",
        "outputId": "02cfd344-9860-45bd-b2b1-bb3dc8a6287d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, tokenizers, transformers, sentence-transformers, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.3\n",
            "    Uninstalling transformers-4.46.3:\n",
            "      Successfully uninstalled transformers-4.46.3\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 3.2.1\n",
            "    Uninstalling sentence-transformers-3.2.1:\n",
            "      Successfully uninstalled sentence-transformers-3.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 sentence-transformers-3.3.1 tokenizers-0.21.0 transformers-4.47.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade sentence-transformers transformers datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t8BeAZLih2K",
        "outputId": "a1a8b22f-5818-409a-a265-2b5da8f7c2e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing chunk 1/3\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: in_progress\n",
            "Job for chunk 0 status: finalizing\n",
            "Job for chunk 0 completed\n",
            "Processing chunk 2/3\n",
            "Job for chunk 1 status: validating\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: in_progress\n",
            "Job for chunk 1 status: finalizing\n",
            "Job for chunk 1 completed\n",
            "Processing chunk 3/3\n",
            "Job for chunk 2 status: validating\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: in_progress\n",
            "Job for chunk 2 status: finalizing\n",
            "Job for chunk 2 completed\n",
            "Embeddings generation and saving completed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from openai import OpenAI\n",
        "from openai import APIError, RateLimitError, APIConnectionError\n",
        "\n",
        "OPENAI_API_KEY = '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'\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "client = OpenAI()\n",
        "\n",
        "def truncate_text(text, max_words=512):\n",
        "    words = text.split()\n",
        "    return ' '.join(words[:max_words])\n",
        "\n",
        "def process_chunk(chunk, chunk_id):\n",
        "\n",
        "    os.makedirs(\"batch_files\", exist_ok=True)\n",
        "    chunk['abstract'] = chunk['abstract'].apply(lambda x: truncate_text(str(x), max_words=512))\n",
        "    chunk['combined_text'] = 'Judul: ' + chunk['title'] + '\\n' + 'Abstrak:' + chunk['abstract'] + '\\n' + 'Keyword:' + chunk['keyword'].fillna('')\n",
        "\n",
        "    batch_file = f'batch_files/batch_{chunk_id}.jsonl'\n",
        "    with open(batch_file, 'w') as f:\n",
        "        for index, row in chunk.iterrows():\n",
        "            payload = {\n",
        "                \"custom_id\": f\"custom_id_{index}\",\n",
        "                \"method\": \"POST\",\n",
        "                \"url\": \"/v1/embeddings\",\n",
        "                \"body\": {\n",
        "                    \"input\": row[\"combined_text\"],\n",
        "                    \"model\": \"text-embedding-3-small\",\n",
        "                    \"encoding_format\": \"float\",\n",
        "                    \"dimension\":1024\n",
        "                }\n",
        "            }\n",
        "            f.write(json.dumps(payload) + '\\n')\n",
        "\n",
        "    return batch_file\n",
        "\n",
        "def create_and_monitor_job(file, chunk_id):\n",
        "    max_retries = 5\n",
        "    while max_retries > 0:\n",
        "        try:\n",
        "            uploaded_file = client.files.create(file=open(file, \"rb\"), purpose=\"batch\")\n",
        "            job = client.batches.create(\n",
        "                input_file_id=uploaded_file.id,\n",
        "                endpoint=\"/v1/embeddings\",\n",
        "                completion_window=\"24h\",\n",
        "                metadata={\"description\": f\"Embedding batch job for chunk {chunk_id}\"}\n",
        "            )\n",
        "\n",
        "            while True:\n",
        "                job_status = client.batches.retrieve(job.id)\n",
        "                if job_status.status == \"completed\":\n",
        "                    print(f\"Job for chunk {chunk_id} completed\")\n",
        "                    # Log the raw API response\n",
        "                    with open(f'raw_response_chunk_{chunk_id}.txt', 'w') as f:\n",
        "                        f.write(client.files.content(job_status.output_file_id).text)\n",
        "                    return process_job_results(job_status, chunk_id)\n",
        "                elif job_status.status == \"failed\":\n",
        "                    print(f\"Job for chunk {chunk_id} failed. Retrying...\")\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"Job for chunk {chunk_id} status: {job_status.status}\")\n",
        "                    time.sleep(60)\n",
        "\n",
        "        except (APIError, RateLimitError, APIConnectionError) as e:\n",
        "            print(f\"Error processing chunk {chunk_id}: {str(e)}\")\n",
        "            time.sleep(60)\n",
        "\n",
        "        max_retries -= 1\n",
        "\n",
        "    print(f\"Failed to process chunk {chunk_id} after multiple retries\")\n",
        "    return None\n",
        "\n",
        "def process_job_results(job, chunk_id):\n",
        "    output_file = client.files.content(job.output_file_id).text\n",
        "    chunk_embeddings = []\n",
        "    for line_number, line in enumerate(output_file.split('\\n'), 1):\n",
        "        if line:\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                custom_id = data['custom_id']\n",
        "                embedding = data['response']['body']['data'][0]['embedding']\n",
        "                index = int(custom_id.split('_')[-1])\n",
        "                chunk_embeddings.append((index, embedding))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"JSON decode error in chunk {chunk_id}, line {line_number}: {str(e)}\")\n",
        "                print(f\"Problematic line: {line[:100]}...\")  # Print first 100 characters of the line\n",
        "            except KeyError as e:\n",
        "                print(f\"Key error in chunk {chunk_id}, line {line_number}: {str(e)}\")\n",
        "                print(f\"Data structure: {data}\")\n",
        "\n",
        "    return chunk_embeddings\n",
        "\n",
        "def get_last_processed_chunk():\n",
        "    processed_files = [f for f in os.listdir() if f.startswith('embeddings_chunk_') and f.endswith('.csv')]\n",
        "    if not processed_files:\n",
        "        return -1\n",
        "    chunk_numbers = [int(f.split('_')[-1].split('.')[0]) for f in processed_files]\n",
        "    return max(chunk_numbers)\n",
        "\n",
        "def load_existing_embeddings():\n",
        "    last_chunk = get_last_processed_chunk()\n",
        "    all_embeddings = []\n",
        "    for i in range(last_chunk + 1):\n",
        "        chunk_file = f'embeddings_chunk_{i}.csv'\n",
        "        if os.path.exists(chunk_file):\n",
        "            chunk_embeddings = pd.read_csv(chunk_file)\n",
        "            all_embeddings.extend(list(chunk_embeddings.itertuples(index=False, name=None)))\n",
        "    return all_embeddings\n",
        "\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv('/content/drive/MyDrive/dataset bangkit/dataset.csv')\n",
        "    chunk_size = 1000  # Process 1000 rows at a time\n",
        "    num_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size > 0 else 0)\n",
        "\n",
        "    # Load existing embeddings\n",
        "    all_embeddings = load_existing_embeddings()\n",
        "    last_processed_chunk = get_last_processed_chunk()\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        if i <= last_processed_chunk:\n",
        "            print(f\"Chunk {i} already processed. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing chunk {i+1}/{num_chunks}\")\n",
        "        start_idx = i * chunk_size\n",
        "        end_idx = min((i + 1) * chunk_size, len(df))\n",
        "        chunk = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "        batch_file = process_chunk(chunk, i)\n",
        "        chunk_embeddings = create_and_monitor_job(batch_file, i)\n",
        "\n",
        "        if chunk_embeddings:\n",
        "            all_embeddings.extend(chunk_embeddings)\n",
        "\n",
        "        # Save intermediate results\n",
        "        pd.DataFrame(chunk_embeddings, columns=['index', 'embedding']).to_csv(f'embeddings_chunk_{i}.csv', index=False)\n",
        "\n",
        "    # Combine all embeddings\n",
        "    final_embeddings = pd.DataFrame(all_embeddings, columns=['index', 'embedding'])\n",
        "    final_embeddings['index'] = final_embeddings['index'].astype(int)\n",
        "    final_embeddings.sort_values('index', inplace=True)\n",
        "\n",
        "    # Merge embeddings with original dataframe\n",
        "    df = df.merge(final_embeddings, left_index=True, right_on='index', how='left')\n",
        "\n",
        "    # Save the final results\n",
        "    df.to_csv('dataset_with_embedding.csv', index=False)\n",
        "    print(\"Embeddings generation and saving completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYAcbR2qmpHs",
        "outputId": "162a7ff4-13f5-424a-d942-81285a214659"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All triplets have been generated and saved.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# Load the dataset in chunks to handle large data\n",
        "chunksize = 1000  # Adjust based on memory constraints\n",
        "output_file = 'triplets.csv'\n",
        "\n",
        "# Function to truncate text\n",
        "def truncate_text(text, max_words=512):\n",
        "    words = text.split()\n",
        "    return ' '.join(words[:max_words])\n",
        "\n",
        "# Parameters for triplet generation\n",
        "top_n_positives = 1  # Number of positive samples per anchor\n",
        "top_n_negatives = 1  # Number of negative samples per anchor\n",
        "\n",
        "# Step 1: Load all embeddings and texts into memory for cross-batch comparison\n",
        "all_embeddings = []\n",
        "all_texts = []\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/dataset bangkit/dataset_with_embeddings.csv\", chunksize=chunksize)\n",
        "\n",
        "# Loop through each chunk and gather all embeddings and texts\n",
        "for df_chunk in df:\n",
        "    # Convert embeddings from strings to numpy arrays\n",
        "    df_chunk['embedding'] = df_chunk['embedding'].apply(lambda x: np.fromstring(x.strip('[]'), sep=','))\n",
        "    df_chunk['abstract'] = df_chunk['abstract'].apply(lambda x: truncate_text(str(x), max_words=512))\n",
        "    df_chunk['combined_text'] = 'Judul: ' + df_chunk['title'] + '\\n' + 'Abstrak: ' + df_chunk['abstract'] + '\\n' + 'Keyword: ' + df_chunk['keyword'].fillna('')\n",
        "\n",
        "    all_embeddings.append(np.array(df_chunk['embedding'].tolist()))  # Store embeddings\n",
        "    all_texts.extend(df_chunk['combined_text'].tolist())  # Store combined text\n",
        "\n",
        "# Combine all embeddings and texts into a single array\n",
        "all_embeddings = np.vstack(all_embeddings)  # Stack all embeddings vertically\n",
        "all_texts = np.array(all_texts)\n",
        "\n",
        "# Step 2: Now loop through each row and calculate cosine similarity across all rows\n",
        "triplets = []\n",
        "\n",
        "# Loop through each row to calculate its similarity against all rows\n",
        "for i in range(len(all_embeddings)):\n",
        "    # Calculate cosine similarity between the i-th embedding and all embeddings\n",
        "    sim_scores = cosine_similarity([all_embeddings[i]], all_embeddings)[0]\n",
        "\n",
        "    # Get indices of the top N most similar (positive) and least similar (negative) rows\n",
        "    positive_indices = np.argsort(sim_scores)[::-1][1:top_n_positives + 1]  # Top similar, excluding self\n",
        "    negative_indices = np.argsort(sim_scores)[:top_n_negatives]  # Least similar\n",
        "\n",
        "    anchor = all_texts[i]\n",
        "    positives = all_texts[positive_indices]\n",
        "    negatives = all_texts[negative_indices]\n",
        "\n",
        "    # Create triplets (anchor, positive, negative)\n",
        "    for pos in positives:\n",
        "        for neg in negatives:\n",
        "            triplets.append((anchor, pos, neg))\n",
        "\n",
        "    # Save triplets in batches to avoid memory issues\n",
        "    if len(triplets) > 1000:  # Save every 1000 triplets\n",
        "        triplets_df = pd.DataFrame(triplets, columns=['anchor', 'positive', 'negative'])\n",
        "        triplets_df.to_csv(output_file, mode='a', index=False, header=not pd.io.common.file_exists(output_file))\n",
        "        triplets = []  # Clear triplets from memory\n",
        "\n",
        "# Save any remaining triplets\n",
        "if triplets:\n",
        "    triplets_df = pd.DataFrame(triplets, columns=['anchor', 'positive', 'negative'])\n",
        "    triplets_df.to_csv(output_file, mode='a', index=False, header=not pd.io.common.file_exists(output_file))\n",
        "\n",
        "print(\"All triplets have been generated and saved.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}